import os
import sys
import shutil
import whisper
import torch
import random
from pathlib import Path
from typing import List, Tuple, Optional, Dict
from moviepy.editor import VideoFileClip, concatenate_videoclips
import imageio_ffmpeg

# [í™˜ê²½ ì„¤ì •] WinError 1114 ë° FFmpeg ê²½ë¡œ ì„¤ì •
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_NUM_THREADS"] = "1"

ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
ffmpeg_dir = os.path.dirname(ffmpeg_exe)
os.environ["PATH"] += os.pathsep + ffmpeg_dir

# Windows ffmpeg ë°”ì´ë„ˆë¦¬ ë³µì‚¬ (ì•ˆì „ì¥ì¹˜)
if sys.platform == "win32" and shutil.which("ffmpeg") is None:
    try:
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        bin_dir = os.path.join(project_root, "bin")
        os.makedirs(bin_dir, exist_ok=True)
        target_ffmpeg = os.path.join(bin_dir, "ffmpeg.exe")
        if not os.path.exists(target_ffmpeg) or os.path.getsize(target_ffmpeg) != os.path.getsize(ffmpeg_exe):
            shutil.copy2(ffmpeg_exe, target_ffmpeg)
        os.environ["PATH"] = bin_dir + os.pathsep + os.environ["PATH"]
    except Exception as e:
        print(f"Warning: Failed to setup local ffmpeg: {e}")

class MeariProcessor:
    _model_cache = {}

    def __init__(
        self,
        model_name: str = "tiny",
        device: Optional[str] = None,
        triggers: Optional[List[str]] = None,
    ) -> None:
        self.model_name = model_name
        
        # [Device Auto-Detection]
        if device is None:
            if torch.cuda.is_available():
                self.device = "cuda"
                print(f"ğŸš€ CUDA GPU Detected: {torch.cuda.get_device_name(0)}")
            else:
                self.device = "cpu"
                print("âš ï¸ No GPU detected, using CPU (slower)")
        else:
            self.device = device
            
        self.triggers = triggers or ["ì‹œì‘", "í•˜ë‚˜ë‘˜ì…‹", "ë‘˜ì…‹"]
        self.model = None

    def _ensure_model(self) -> None:
        if self.model is None:
            # ìºì‹œëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
            cache_key = (self.model_name, self.device)
            if cache_key in MeariProcessor._model_cache:
                print(f"Loading Whisper model '{self.model_name}' from cache...")
                self.model = MeariProcessor._model_cache[cache_key]
            else:
                print(f"Loading Whisper model '{self.model_name}' on {self.device}...")
                self.model = whisper.load_model(self.model_name, device=self.device)
                MeariProcessor._model_cache[cache_key] = self.model

    def transcribe(
        self,
        video_path: str | Path,
        language: str = "ko",
    ) -> dict:
        self._ensure_model()
        
        # [Whisper Prompt Engineering]
        # ëª¨ë¸ì—ê²Œ ì´ëŸ° ë‹¨ì–´ê°€ ë‚˜ì˜¬ ê±°ë¼ê³  ë¯¸ë¦¬ 'ê·€ë”'ì„ í•´ì¤Œ
        # ì¡ìŒì´ ë§ê±°ë‚˜ ë°œí™”ê°€ ì§§ì€ ê²½ìš°ì—ë„ ë†“ì¹˜ì§€ ì•Šë„ë¡ ìœ ë„
        # "ì¡¸ì—…", "í•™êµ" ë“± ì•„ì´ë“¤ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
        initial_prompt = "ì„ ìƒë‹˜ ëª©ì†Œë¦¬. ìœ ì¹˜ì› ìˆ˜ì—…. ì‹œì‘. í•˜ë‚˜ë‘˜ì…‹. ë‘˜ì…‹. ì§‘ì¤‘í•˜ì„¸ìš”. ì. ë„¤. ì¡¸ì—…. í•™êµ. ì…í•™. ì•ˆë…•."
        
        # [Optimization Settings]
        use_fp16 = (self.device == "cuda")
        beam_size = 1 if self.device == "cpu" else 5 # CPUëŠ” ì†ë„ ìš°ì„ (Greedy), GPUëŠ” ì •í™•ë„ ìš°ì„ (Beam)
        
        try:
            result = self.model.transcribe(
                str(video_path),
                language=language,
                verbose=False,
                initial_prompt=initial_prompt,    # í•µì‹¬: íŒíŠ¸ ì œê³µ
                word_timestamps=True,             # í•µì‹¬: ë‹¨ì–´ ë‹¨ìœ„ ì‹œê°„ í™œì„±í™” (ì •ë°€ë„ í–¥ìƒ)
                condition_on_previous_text=False, # í™˜ê° ë°©ì§€
                fp16=use_fp16,                    # GPU ê°€ì† (CPUëŠ” False)
                beam_size=beam_size,              # íƒìƒ‰ í­ ì¡°ì ˆ
                
                # [Hallucination Prevention]
                # ì¡ìŒì„ ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ë¡œ ì¸ì‹í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°
                compression_ratio_threshold=2.4,  # ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸(ë‚˜ë‚˜ë‚˜...) ë¬´ì‹œ
                logprob_threshold=-1.0,           # í™•ì‹ ì´ ë‚®ì€ êµ¬ê°„ ë¬´ì‹œ
                no_speech_threshold=0.6           # ë§ì†Œë¦¬ê°€ ì—†ëŠ” êµ¬ê°„ ë¬´ì‹œ
            )
        except Exception as e:
            print(f"Transcription failed with default settings: {e}")
            print("Retrying with fallback settings (fp16=False, beam_size=1)...")
            try:
                result = self.model.transcribe(
                    str(video_path),
                    language=language,
                    verbose=False,
                    fp16=False,
                    beam_size=1, # RetryëŠ” ë¬´ì¡°ê±´ Greedy Search
                    initial_prompt=initial_prompt,
                    word_timestamps=True,
                    condition_on_previous_text=False,
                    
                    # [Hallucination Prevention] - Retryì—ë„ ë™ì¼ ì ìš©
                    compression_ratio_threshold=2.4,
                    logprob_threshold=-1.0,
                    no_speech_threshold=0.6
                )
            except Exception as e2:
                print(f"Transcription failed again: {e2}")
                raise e2
                
        return result

    def find_trigger_segments(self, segments: List[dict], triggers: Optional[List[str]] = None) -> List[dict]:
        """
        ë‹¨ì–´(Word) ë‹¨ìœ„ë¡œ ì •ë°€í•˜ê²Œ íŠ¸ë¦¬ê±°ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        ì‹œê°„ìƒìœ¼ë¡œ ë©€ë¦¬ ë–¨ì–´ì§„ íŠ¸ë¦¬ê±° ë‹¨ì–´ë“¤ì€ ë³„ê°œì˜ êµ¬ê°„ìœ¼ë¡œ ë¶„ë¦¬(Clustering)í•©ë‹ˆë‹¤.
        """
        use_triggers = triggers or self.triggers
        found = []
        num_map = {"1": "í•˜ë‚˜", "2": "ë‘˜", "3": "ì…‹"} # ìˆ«ì ë³€í™˜ ë§µ

        for seg in segments:
            # 1. ì •ë°€ ëª¨ë“œ: Whisperê°€ 'ë‹¨ì–´ ì •ë³´(words)'ë¥¼ ì¤¬ì„ ë•Œ
            if "words" in seg:
                trigger_words = []
                # 1ì°¨ í•„í„°ë§: ì„¸ê·¸ë¨¼íŠ¸ ë‚´ì˜ ëª¨ë“  íŠ¸ë¦¬ê±° ë‹¨ì–´ ìˆ˜ì§‘
                for word_info in seg["words"]:
                    word_text = word_info.get("word", "").strip()
                    # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ìˆ«ì ë³€í™˜
                    clean_text = "".join(c for c in word_text if c.isalnum())
                    clean_text = num_map.get(clean_text, clean_text)
                    
                    # íŠ¸ë¦¬ê±° ë‹¨ì–´ì¸ì§€ í™•ì¸
                    for trigger in use_triggers:
                        if trigger in clean_text or clean_text in trigger:
                            trigger_words.append(word_info)
                            break
                
                # 2ì°¨ í´ëŸ¬ìŠ¤í„°ë§: ì‹œê°„ì°¨ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë£¹ ë¶„ë¦¬
                if trigger_words:
                    groups = []
                    current_group = []
                    cluster_threshold = 0.8  # íŠ¸ë¦¬ê±° ë‹¨ì–´ ê°„ ë¶„ë¦¬ ì„ê³„ê°’ (0.8ì´ˆë¡œ ìƒí–¥ ì¡°ì •)
                    
                    for word in trigger_words:
                        if not current_group:
                            current_group.append(word)
                        else:
                            last_word = current_group[-1]
                            # í˜„ì¬ ë‹¨ì–´ ì‹œì‘ - ì´ì „ ë‹¨ì–´ ë ì‹œê°„ ì°¨ì´ê°€ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ê°™ì€ ê·¸ë£¹
                            if float(word["start"]) - float(last_word["end"]) < cluster_threshold:
                                current_group.append(word)
                            else:
                                # ì„ê³„ê°’ ì´ìƒ ì°¨ì´ë‚˜ë©´ ê¸°ì¡´ ê·¸ë£¹ ì €ì¥í•˜ê³  ìƒˆ ê·¸ë£¹ ì‹œì‘
                                groups.append(current_group)
                                current_group = [word]
                    
                    # ë§ˆì§€ë§‰ ê·¸ë£¹ ì €ì¥
                    if current_group:
                        groups.append(current_group)
                    
                    # ê° ê·¸ë£¹ë³„ë¡œ ê²°ê³¼ ìƒì„±
                    for group in groups:
                        start_time = float(group[0]["start"])
                        end_time = float(group[-1]["end"])
                        detected_text = " ".join([w["word"] for w in group])
                        
                        # [Dynamic Padding] íŠ¸ë¦¬ê±° ë‹¨ì–´ë³„ ë§ì¶¤ íŒ¨ë”©
                        # ê¸°ë³¸ê°’ì€ ì§§ê²Œ(0.1), ê¸¸ê²Œ ë„ëŠ” ë‹¨ì–´("ë‹¤ì‹œ")ëŠ” ê¸¸ê²Œ(0.8)
                        current_end_padding = 0.1
                        
                        if "ë‹¤ì‹œ" in detected_text:
                            current_end_padding = 0.8
                        elif "ì" in detected_text: # "ì~" ê°™ì€ ê²½ìš°
                            current_end_padding = 0.5
                            
                        # ì‹ ë¢°ë„ ë° ìƒíƒœ ê²°ì •
                        confidence = 1.0
                        status = "confirmed" if confidence >= 0.9 else "candidate"

                        found.append({
                            "text": detected_text,
                            "word": detected_text, # Alias for UI compatibility
                            "start": max(0, start_time - 0.2), # ì‹œì‘ íŒ¨ë”©ì€ 0.2ë¡œ ê³ ì •
                            "end": end_time + current_end_padding, # ë íŒ¨ë”© ê°€ë³€ ì ìš©
                            "confidence": confidence,
                            "status": status
                        })
            
            # 2. ì¼ë°˜ ëª¨ë“œ: ë‹¨ì–´ ì •ë³´ê°€ ì—†ì„ ë•Œ (ì˜ˆë¹„ìš©)
            else:
                text = seg.get("text", "")
                clean_text = "".join(c for c in text if c.isalnum())
                for trigger in use_triggers:
                    if trigger in clean_text:
                        # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´(3ì´ˆ ì´ìƒ) íŠ¸ë¦¬ê±°ì¼ í™•ë¥ ì´ ë‚®ìœ¼ë¯€ë¡œ ì•ë¶€ë¶„ 1ì´ˆë§Œ ì‚¬ìš©
                        start = float(seg.get("start", 0))
                        end = float(seg.get("end", 0))
                        if end - start > 3.0:
                            end = start + 1.0
                            
                        # ì‹ ë¢°ë„ ë° ìƒíƒœ ê²°ì • (ì¼ë°˜ ëª¨ë“œëŠ” ì‹ ë¢°ë„ ë‚®ìŒ)
                        confidence = 0.8
                        status = "confirmed" if confidence >= 0.9 else "candidate"

                        found.append({
                            "text": text,
                            "word": text,
                            "start": start,
                            "end": end,
                            "confidence": confidence,
                            "status": status
                        })
                        break
        return found

    def calculate_intervals(
        self,
        trigger_segments: List[dict],
        total_duration: float,
        include_trigger: bool = False,
    ) -> List[Tuple[float, float]]:
        # íŠ¸ë¦¬ê±° êµ¬ê°„ì„ 'ì‚­ì œ(Red)'í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ 'ë³´ì¡´(Green)'í•˜ëŠ” ë¡œì§
        valid_clips = []
        if not trigger_segments:
             return [(0.0, total_duration)]

        sorted_triggers = sorted(trigger_segments, key=lambda x: float(x.get("start", 0)))
        last_end = 0.0
        
        for seg in sorted_triggers:
            start = float(seg.get("start", 0))
            end = float(seg.get("end", 0))
            
            if start > last_end:
                valid_clips.append((last_end, start))
            
            if include_trigger:
                last_end = start # íŠ¸ë¦¬ê±° í¬í•¨ (ì‚­ì œ ì•ˆ í•¨)
            else:
                last_end = end   # íŠ¸ë¦¬ê±° ì‚­ì œ
                
        if last_end < total_duration:
            valid_clips.append((last_end, total_duration))
            
        return valid_clips

    def _detect_speakers(self, total_duration: float) -> Tuple[List[dict], List[dict]]:
        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° (ì‹¤ì œ ë¶„ì„ ì•„ë‹˜ - ì†ë„ ì˜í–¥ ì—†ìŒ)
        random.seed(42)
        speaker_defs = [
            {"id": "spk_01", "name": "ì„ ìƒë‹˜ ëª©ì†Œë¦¬ (í›„ë³´)", "is_adult": True, "weight": 0.45},
            {"id": "spk_02", "name": "ì•„ì´ë“¤ ëª©ì†Œë¦¬", "is_adult": False, "weight": 0.35},
            {"id": "spk_03", "name": "ëª©ì†Œë¦¬ 2 (ì„±ì¸)", "is_adult": True, "weight": 0.15},
        ]
        speaker_segments = []
        current_time = 0.0
        
        while current_time < total_duration:
            seg_duration = random.uniform(2.0, 10.0)
            if current_time + seg_duration > total_duration:
                seg_duration = total_duration - current_time
            
            chosen = random.choices(speaker_defs, weights=[s["weight"] for s in speaker_defs])[0]
            speaker_segments.append({
                "start": current_time,
                "end": current_time + seg_duration,
                "speaker_id": chosen["id"]
            })
            current_time += seg_duration
            
        return speaker_defs, speaker_segments

    def filter_triggers_by_speaker(self, triggers, speaker_segments, selected_ids, tolerance=1.0):
        # [ê´€ëŒ€í•¨ ëª¨ë“œ] ì• ë§¤í•˜ë©´ ì‚´ë ¤ë‘ëŠ” ë¡œì§
        if not selected_ids: return triggers
        valid = []
        for trig in triggers:
            trig_start = float(trig.get("start", 0))
            trig_end = float(trig.get("end", 0))
            check_start = max(0, trig_start - tolerance)
            check_end = trig_end + tolerance
            
            overlapping = set()
            for seg in speaker_segments:
                if max(check_start, seg["start"]) < min(check_end, seg["end"]):
                    overlapping.add(seg["speaker_id"])
            
            # ëª©ì†Œë¦¬ ì •ë³´ê°€ ì—†ê±°ë‚˜, ì„ íƒëœ ëª©ì†Œë¦¬ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ìœ ì§€
            if not overlapping or any(sid in selected_ids for sid in overlapping):
                valid.append(trig)
            # í™•ì‹¤íˆ ë‹¤ë¥¸ ëª©ì†Œë¦¬(ì•„ì´ë“¤)ë§Œ ìˆì„ ë•Œë§Œ ì œê±°
        return valid

    def analyze_video(
        self,
        video_path: str | Path,
        language: str = "ko",
        triggers: Optional[List[str]] = None,
        include_trigger: bool = False,
    ) -> Tuple[List[Tuple[float, float]], float, List[dict], List[dict], List[dict]]:
        
        path_str = str(video_path)
        
        # [ì†ë„ ìµœì í™”] ì˜ìƒ ê¸¸ì´ë§Œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¤ê³  ì¦‰ì‹œ í•´ì œ (ì¸ì½”ë”© ë°©ì§€)
        try:
            with VideoFileClip(path_str) as clip:
                total_duration = float(clip.duration)
        except Exception as e:
            print(f"Error reading video duration: {e}")
            total_duration = 0.0

        # [Whisper ì‹¤í–‰] ë¹„ë””ì˜¤ ì¸ì½”ë”© ì—†ì´ ì˜¤ë””ì˜¤ë§Œ ë‚´ë¶€ ì¶”ì¶œí•˜ì—¬ ë¹ ë¥´ê²Œ ë¶„ì„
        # write_videofileì´ë‚˜ ì˜¤ë””ì˜¤ ë³€í™˜ ê³¼ì • ì—†ìŒ
        result = self.transcribe(path_str, language=language)
        segments = result.get("segments", [])

        # 3. íŠ¸ë¦¬ê±° ì°¾ê¸° (í…ìŠ¤íŠ¸ ë¶„ì„)
        all_trigger_segments = self.find_trigger_segments(segments, triggers=triggers)
        
        # 4. ëª©ì†Œë¦¬ ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜)
        speakers_summary, speaker_segments = self._detect_speakers(total_duration)
        
        # 5. êµ¬ê°„ ê³„ì‚°
        valid_clips = self.calculate_intervals(all_trigger_segments, total_duration, include_trigger)
                
        return valid_clips, total_duration, all_trigger_segments, speakers_summary, speaker_segments

    def export_with_intervals(
        self,
        video_path: str | Path,
        output_path: str | Path,
        intervals: List[Tuple[float, float]],
        crossfade: float = 0.2,
        min_segment_duration: float = 0.5,
        fps: Optional[int] = None,
        codec: str = "libx264", # ê¸°ë³¸ê°’ì€ ìœ ì§€í•˜ë˜ ë‚´ë¶€ì—ì„œ ë¬´ì‹œí•˜ê±°ë‚˜ ì¬ì„¤ì •
        audio_codec: str = "aac",
    ) -> Optional[Path]:
        if not intervals: return None
        path_str = str(video_path)
        output = Path(output_path)
        output.parent.mkdir(parents=True, exist_ok=True)

        try:
            # [Smart Rendering Logic]
            # ë§Œì•½ crossfadeê°€ 0ì´ê³ , ì½”ë± ë³€í™˜ì´ í•„ìš” ì—†ë‹¤ë©´ ìŠ¤íŠ¸ë¦¼ ë³µì‚¬(Copy Stream) ì‹œë„
            # ì´ëŠ” ì¸ì½”ë”©ì„ ì•„ì˜ˆ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì†ë„ê°€ ìˆ˜ì‹­ ë°° ë¹ ë¦„
            # ë‹¨, í‚¤í”„ë ˆì„ ë¬¸ì œë¡œ ì •í™•í•œ ì»·ì´ ì•ˆ ë  ìˆ˜ ìˆì–´ ì œí•œì ìœ¼ë¡œ ì‚¬ìš©
            use_smart_rendering = (crossfade <= 0)
            
            if use_smart_rendering:
                print("âš¡ ìŠ¤ë§ˆíŠ¸ ë Œë”ë§(Stream Copy) ì‹œë„ ì¤‘... (ì´ˆê³ ì† ëª¨ë“œ)")
                try:
                    import subprocess
                    
                    # ffmpeg filter_complexë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ ë³µì‚¬ ì‹œë„
                    # concat demuxer ì‚¬ìš©ì„ ìœ„í•´ ì„ì‹œ íŒŒì¼ ëª©ë¡ ìƒì„±
                    temp_list_path = output.with_suffix('.txt')
                    with open(temp_list_path, 'w', encoding='utf-8') as f:
                        for start, end in intervals:
                            if end - start < min_segment_duration: continue
                            # inpoint/outpoint ì‚¬ìš©
                            f.write(f"file '{path_str.replace(os.sep, '/')}'\n")
                            f.write(f"inpoint {start}\n")
                            f.write(f"outpoint {end}\n")
                    
                    # ffmpeg concat ì‹¤í–‰
                    cmd = [
                        imageio_ffmpeg.get_ffmpeg_exe(),
                        "-f", "concat",
                        "-safe", "0",
                        "-i", str(temp_list_path),
                        "-c", "copy",  # í•µì‹¬: ì¸ì½”ë”© ì—†ì´ ë³µì‚¬
                        "-y",
                        str(output)
                    ]
                    
                    subprocess.run(cmd, check=True, capture_output=True)
                    os.remove(temp_list_path)
                    print("âœ… ìŠ¤ë§ˆíŠ¸ ë Œë”ë§ ì„±ê³µ!")
                    return output
                    
                except Exception as e:
                    print(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ë Œë”ë§ ì‹¤íŒ¨ (ì¬ì¸ì½”ë”©ìœ¼ë¡œ ì „í™˜): {e}")
                    if 'temp_list_path' in locals() and os.path.exists(temp_list_path):
                        os.remove(temp_list_path)

            # [Standard Rendering with Hardware Acceleration]
            with VideoFileClip(path_str) as clip:
                child_clips = []
                for start, end in intervals:
                    duration = end - start
                    if duration < min_segment_duration: continue
                    
                    subclip = clip.subclip(start, end)
                    if crossfade > 0:
                        subclip = subclip.audio_fadein(crossfade).audio_fadeout(crossfade)
                    child_clips.append(subclip)

                if not child_clips: return None

                final_clip = concatenate_videoclips(child_clips, method="compose", padding=-crossfade)
                
                # [1ë‹¨ê³„] GPU ê°€ì† ì‹œë„ (NVIDIA NVENC)
                print("ğŸš€ GPU ê°€ì† ì¸ì½”ë”© ì‹œë„ ì¤‘... (h264_nvenc)")
                try:
                    final_clip.write_videofile(
                        str(output),
                        codec="h264_nvenc",     # GPU ì½”ë±
                        audio_codec=audio_codec,
                        fps=fps or 24,
                        preset="p1",            # ê°€ì¥ ë¹ ë¦„
                        threads=8,
                        ffmpeg_params=["-rc", "constqp", "-qp", "23"],
                        logger="bar"
                    )
                except Exception as e1:
                    print(f"âš ï¸ NVENC ì¸ì½”ë”© ì‹¤íŒ¨: {e1}")
                    
                    # [2ë‹¨ê³„] Intel QuickSync (QSV) ì‹œë„
                    print("ğŸš€ Intel QSV ê°€ì† ì¸ì½”ë”© ì‹œë„ ì¤‘... (h264_qsv)")
                    try:
                        final_clip.write_videofile(
                            str(output),
                            codec="h264_qsv",
                            audio_codec=audio_codec,
                            fps=fps or 24,
                            preset="veryfast",
                            threads=8,
                            ffmpeg_params=["-global_quality", "23"],
                            logger="bar"
                        )
                    except Exception as e2:
                        print(f"âš ï¸ QSV ì¸ì½”ë”© ì‹¤íŒ¨: {e2}")
                        
                        # [3ë‹¨ê³„] AMD AMF ì‹œë„
                        print("ğŸš€ AMD AMF ê°€ì† ì¸ì½”ë”© ì‹œë„ ì¤‘... (h264_amf)")
                        try:
                            final_clip.write_videofile(
                                str(output),
                                codec="h264_amf",
                                audio_codec=audio_codec,
                                fps=fps or 24,
                                preset="speed",
                                threads=8,
                                logger="bar"
                            )
                        except Exception as e3:
                            print(f"âš ï¸ AMF ì¸ì½”ë”© ì‹¤íŒ¨: {e3}")
                            
                            # [4ë‹¨ê³„] CPU ì´ˆê³ ì† ëª¨ë“œ (Fallback)
                            print("ğŸ¢ CPU ì¸ì½”ë”©ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤. (libx264 ultrafast)")
                            final_clip.write_videofile(
                                str(output),
                                codec="libx264",
                                audio_codec=audio_codec,
                                fps=fps or 24,
                                preset="ultrafast",     # CPU ìµœê³  ì†ë„
                                threads=os.cpu_count() or 4, # ê°€ìš© ìŠ¤ë ˆë“œ ìµœëŒ€ í™œìš©
                                ffmpeg_params=["-crf", "28", "-tune", "zerolatency"], # crf 28ë¡œ ì†ë„ ìš°ì„ 
                                logger="bar"
                            )
                    
                final_clip.close()
                
            return output
            
        except Exception as e:
            print(f"Export failed: {e}")
            return None
